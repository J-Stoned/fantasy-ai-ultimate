apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-engine
  namespace: fantasy-ai
  labels:
    app: ml-engine
    component: ai
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ml-engine
  template:
    metadata:
      labels:
        app: ml-engine
        component: ai
    spec:
      nodeSelector:
        gpu: "true"  # Schedule on GPU nodes
      containers:
      - name: ml-engine
        image: fantasy-ai/ml-engine:latest  # Replace with your registry
        imagePullPolicy: Always
        command: ["node", "lib/ml/ProductionMLEngine.js"]
        envFrom:
        - configMapRef:
            name: fantasy-ai-config
        - secretRef:
            name: fantasy-ai-secrets
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
            nvidia.com/gpu: 1  # Request 1 GPU
          limits:
            memory: "4Gi"
            cpu: "2"
            nvidia.com/gpu: 1
        volumeMounts:
        - name: ml-models
          mountPath: /models
        - name: ml-cache
          mountPath: /cache
      volumes:
      - name: ml-models
        persistentVolumeClaim:
          claimName: ml-models-pvc
      - name: ml-cache
        emptyDir:
          sizeLimit: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: ml-engine-service
  namespace: fantasy-ai
spec:
  selector:
    app: ml-engine
  ports:
    - port: 9090
      targetPort: 9090
      name: metrics
    - port: 8000
      targetPort: 8000
      name: grpc
  type: ClusterIP