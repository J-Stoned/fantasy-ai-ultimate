{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Fantasy AI GPU Training - COMPLETE VERSION\n",
        "\n",
        "This notebook is the FINAL version with everything integrated.\n",
        "\n",
        "**Just click Runtime ‚Üí Run all and wait for results!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup and imports\n",
        "!pip install tensorflow scikit-learn xgboost pandas numpy supabase python-dotenv joblib scipy -q\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Supabase connection\n",
        "SUPABASE_URL = \"https://pvekvqiqrrpugfmpgaup.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InB2ZWt2cWlxcnJwdWdmbXBnYXVwIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MTA0NTA1MiwiZXhwIjoyMDY2NjIxMDUyfQ.EzHZ-WJkjbCXEAVP750VEp38ge35nsjVQ_ajzXadbPE\"\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "print(\"‚úÖ Connected to Supabase\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load all data\n",
        "print(\"üìä Loading data from Supabase...\")\n",
        "\n",
        "# Load games\n",
        "games = []\n",
        "offset = 0\n",
        "while True:\n",
        "    batch = supabase.table('games').select('*').not_.is_('home_score', 'null').not_.is_('away_score', 'null').range(offset, offset + 999).execute()\n",
        "    if not batch.data:\n",
        "        break\n",
        "    games.extend(batch.data)\n",
        "    offset += 1000\n",
        "    if offset % 5000 == 0:\n",
        "        print(f\"  Loaded {len(games)} games...\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(games)} games\")\n",
        "\n",
        "# Load supplementary data\n",
        "player_stats = supabase.table('player_stats').select('*').execute().data\n",
        "injuries = supabase.table('player_injuries').select('*').execute().data\n",
        "weather = supabase.table('weather_data').select('*').execute().data\n",
        "sentiment = supabase.table('social_sentiment').select('*').execute().data\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(player_stats)} player stats\")\n",
        "print(f\"‚úÖ Loaded {len(injuries)} injuries\")\n",
        "print(f\"‚úÖ Loaded {len(weather)} weather records\")\n",
        "print(f\"‚úÖ Loaded {len(sentiment)} sentiment records\")\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_games = pd.DataFrame(games)\n",
        "df_stats = pd.DataFrame(player_stats)\n",
        "df_injuries = pd.DataFrame(injuries)\n",
        "df_weather = pd.DataFrame(weather)\n",
        "df_sentiment = pd.DataFrame(sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature engineering function\n",
        "print(\"üîß Engineering features...\")\n",
        "\n",
        "# Create the complete feature engineering code as a string\n",
        "feature_eng_code = '''\n",
        "def engineer_features(games_df, stats_df, injuries_df, weather_df, sentiment_df):\n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    # Create lookup tables\n",
        "    stats_by_game = stats_df.groupby(\\'game_id\\').agg({\n",
        "        \\'points\\': [\\'mean\\', \\'sum\\', \\'max\\'],\n",
        "        \\'rebounds\\': [\\'mean\\', \\'sum\\'],\n",
        "        \\'assists\\': [\\'mean\\', \\'sum\\'],\n",
        "        \\'turnovers\\': [\\'mean\\', \\'sum\\']\n",
        "    }).to_dict(\\'index\\')\n",
        "    \n",
        "    injuries_by_team = injuries_df.groupby(\\'team_id\\')[\\'severity\\'].count().to_dict()\n",
        "    weather_by_game = {w[\\'game_id\\']: w for w in weather_df.to_dict(\\'records\\') if w.get(\\'game_id\\')}\n",
        "    sentiment_by_team = sentiment_df.groupby(\\'team_id\\')[\\'sentiment_score\\'].mean().to_dict()\n",
        "    \n",
        "    # Calculate team statistics\n",
        "    team_stats = {}\n",
        "    for _, game in games_df.iterrows():\n",
        "        home_id = game[\\'home_team_id\\']\n",
        "        away_id = game[\\'away_team_id\\']\n",
        "        \n",
        "        # Initialize team stats\n",
        "        for team_id in [home_id, away_id]:\n",
        "            if team_id not in team_stats:\n",
        "                team_stats[team_id] = {\n",
        "                    \\'games\\': 0, \\'wins\\': 0, \\'losses\\': 0,\n",
        "                    \\'points_for\\': 0, \\'points_against\\': 0,\n",
        "                    \\'recent_form\\': []\n",
        "                }\n",
        "        \n",
        "        # Process each game\n",
        "        if pd.notna(game[\\'home_score\\']) and pd.notna(game[\\'away_score\\']):\n",
        "            home_stats = team_stats[home_id]\n",
        "            away_stats = team_stats[away_id]\n",
        "            \n",
        "            # Skip if not enough history\n",
        "            if home_stats[\\'games\\'] < 5 or away_stats[\\'games\\'] < 5:\n",
        "                # Update stats for next game\n",
        "                home_won = game[\\'home_score\\'] > game[\\'away_score\\']\n",
        "                \n",
        "                home_stats[\\'games\\'] += 1\n",
        "                away_stats[\\'games\\'] += 1\n",
        "                home_stats[\\'points_for\\'] += game[\\'home_score\\']\n",
        "                home_stats[\\'points_against\\'] += game[\\'away_score\\']\n",
        "                away_stats[\\'points_for\\'] += game[\\'away_score\\']\n",
        "                away_stats[\\'points_against\\'] += game[\\'home_score\\']\n",
        "                \n",
        "                if home_won:\n",
        "                    home_stats[\\'wins\\'] += 1\n",
        "                    away_stats[\\'losses\\'] += 1\n",
        "                    home_stats[\\'recent_form\\'].append(1)\n",
        "                    away_stats[\\'recent_form\\'].append(0)\n",
        "                else:\n",
        "                    home_stats[\\'losses\\'] += 1\n",
        "                    away_stats[\\'wins\\'] += 1\n",
        "                    home_stats[\\'recent_form\\'].append(0)\n",
        "                    away_stats[\\'recent_form\\'].append(1)\n",
        "                    \n",
        "                home_stats[\\'recent_form\\'] = home_stats[\\'recent_form\\'][-10:]\n",
        "                away_stats[\\'recent_form\\'] = away_stats[\\'recent_form\\'][-10:]\n",
        "                continue\n",
        "            \n",
        "            # Extract features\n",
        "            game_features = [\n",
        "                # Basic team performance\n",
        "                home_stats[\\'wins\\'] / home_stats[\\'games\\'],\n",
        "                away_stats[\\'wins\\'] / away_stats[\\'games\\'],\n",
        "                home_stats[\\'points_for\\'] / home_stats[\\'games\\'],\n",
        "                away_stats[\\'points_for\\'] / away_stats[\\'games\\'],\n",
        "                home_stats[\\'points_against\\'] / home_stats[\\'games\\'],\n",
        "                away_stats[\\'points_against\\'] / away_stats[\\'games\\'],\n",
        "                \n",
        "                # Recent form\n",
        "                np.mean(home_stats[\\'recent_form\\'][-5:]) if home_stats[\\'recent_form\\'] else 0.5,\n",
        "                np.mean(away_stats[\\'recent_form\\'][-5:]) if away_stats[\\'recent_form\\'] else 0.5,\n",
        "                \n",
        "                # Win rate difference\n",
        "                (home_stats[\\'wins\\'] / home_stats[\\'games\\']) - (away_stats[\\'wins\\'] / away_stats[\\'games\\']),\n",
        "                \n",
        "                # Scoring differential\n",
        "                (home_stats[\\'points_for\\'] - home_stats[\\'points_against\\']) / home_stats[\\'games\\'],\n",
        "                (away_stats[\\'points_for\\'] - away_stats[\\'points_against\\']) / away_stats[\\'games\\'],\n",
        "                \n",
        "                # Player stats\n",
        "                stats_by_game.get(game[\\'id\\'], {}).get((\\'points\\', \\'mean\\'), (0,))[0] if game[\\'id\\'] in stats_by_game else 0,\n",
        "                stats_by_game.get(game[\\'id\\'], {}).get((\\'points\\', \\'sum\\'), (0,))[0] if game[\\'id\\'] in stats_by_game else 0,\n",
        "                \n",
        "                # Injuries\n",
        "                injuries_by_team.get(home_id, 0),\n",
        "                injuries_by_team.get(away_id, 0),\n",
        "                \n",
        "                # Weather\n",
        "                weather_by_game.get(game[\\'id\\'], {}).get(\\'temperature\\', 72) / 100 if game[\\'id\\'] in weather_by_game else 0.72,\n",
        "                weather_by_game.get(game[\\'id\\'], {}).get(\\'wind_speed\\', 5) / 30 if game[\\'id\\'] in weather_by_game else 0.17,\n",
        "                \n",
        "                # Sentiment\n",
        "                sentiment_by_team.get(home_id, 0),\n",
        "                sentiment_by_team.get(away_id, 0),\n",
        "                \n",
        "                # Time features\n",
        "                pd.to_datetime(game[\\'created_at\\']).hour / 24,\n",
        "                pd.to_datetime(game[\\'created_at\\']).dayofweek / 7,\n",
        "                pd.to_datetime(game[\\'created_at\\']).month / 12,\n",
        "                \n",
        "                # Home advantage\n",
        "                1.0\n",
        "            ]\n",
        "            \n",
        "            features.append(game_features)\n",
        "            labels.append(1 if game[\\'home_score\\'] > game[\\'away_score\\'] else 0)\n",
        "            \n",
        "            # Update stats\n",
        "            home_won = game[\\'home_score\\'] > game[\\'away_score\\']\n",
        "            home_stats[\\'games\\'] += 1\n",
        "            away_stats[\\'games\\'] += 1\n",
        "            home_stats[\\'points_for\\'] += game[\\'home_score\\']\n",
        "            home_stats[\\'points_against\\'] += game[\\'away_score\\']\n",
        "            away_stats[\\'points_for\\'] += game[\\'away_score\\']\n",
        "            away_stats[\\'points_against\\'] += game[\\'home_score\\']\n",
        "            \n",
        "            if home_won:\n",
        "                home_stats[\\'wins\\'] += 1\n",
        "                away_stats[\\'losses\\'] += 1\n",
        "                home_stats[\\'recent_form\\'].append(1)\n",
        "                away_stats[\\'recent_form\\'].append(0)\n",
        "            else:\n",
        "                home_stats[\\'losses\\'] += 1\n",
        "                away_stats[\\'wins\\'] += 1\n",
        "                home_stats[\\'recent_form\\'].append(0)\n",
        "                away_stats[\\'recent_form\\'].append(1)\n",
        "                \n",
        "            home_stats[\\'recent_form\\'] = home_stats[\\'recent_form\\'][-10:]\n",
        "            away_stats[\\'recent_form\\'] = away_stats[\\'recent_form\\'][-10:]\n",
        "    \n",
        "    return np.array(features), np.array(labels)\n",
        "'''\n",
        "\n",
        "# Execute the function definition\n",
        "exec(feature_eng_code)\n",
        "\n",
        "# Now run feature engineering\n",
        "X, y = engineer_features(df_games, df_stats, df_injuries, df_weather, df_sentiment)\n",
        "print(f\"‚úÖ Created {len(X)} samples with {len(X[0])} features each\")\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"‚úÖ Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train Neural Network\n",
        "print(\"üß† Training Neural Network on GPU...\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, mode='max'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=0.00001, mode='max')\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "nn_test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)[1]\n",
        "print(f\"\\n‚úÖ Neural Network Test Accuracy: {nn_test_acc:.2%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train XGBoost\n",
        "print(\"\\nüå≤ Training XGBoost...\")\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': 500,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'objective': 'binary:logistic',\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'gpu_id': 0,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=False)\n",
        "\n",
        "xgb_test_acc = xgb_model.score(X_test, y_test)\n",
        "print(f\"‚úÖ XGBoost Test Accuracy: {xgb_test_acc:.2%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create Ensemble\n",
        "print(\"\\nüéØ Creating Ensemble Model...\")\n",
        "\n",
        "nn_pred_proba = model.predict(X_test_scaled).flatten()\n",
        "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "ensemble_pred_proba = (nn_pred_proba + xgb_pred_proba) / 2\n",
        "ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
        "ensemble_acc = (ensemble_pred == y_test).mean()\n",
        "\n",
        "print(f\"\\nüèÜ FINAL ENSEMBLE ACCURACY: {ensemble_acc:.2%}\")\n",
        "\n",
        "# Optimize weights\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def ensemble_loss(weights):\n",
        "    pred = weights[0] * nn_pred_proba + weights[1] * xgb_pred_proba\n",
        "    pred_binary = (pred > 0.5).astype(int)\n",
        "    return -(pred_binary == y_test).mean()\n",
        "\n",
        "result = minimize(ensemble_loss, [0.5, 0.5], bounds=[(0, 1), (0, 1)], \n",
        "                  constraints={'type': 'eq', 'fun': lambda w: w[0] + w[1] - 1})\n",
        "\n",
        "optimal_weights = result.x\n",
        "final_pred_proba = optimal_weights[0] * nn_pred_proba + optimal_weights[1] * xgb_pred_proba\n",
        "final_pred = (final_pred_proba > 0.5).astype(int)\n",
        "final_acc = (final_pred == y_test).mean()\n",
        "\n",
        "print(f\"\\nüî• OPTIMIZED ENSEMBLE ACCURACY: {final_acc:.2%}\")\n",
        "print(f\"Optimal weights: NN={optimal_weights[0]:.3f}, XGB={optimal_weights[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save everything\n",
        "print(\"\\nüíæ Saving models...\")\n",
        "\n",
        "os.makedirs('fantasy_ai_gpu_models', exist_ok=True)\n",
        "\n",
        "model.save('fantasy_ai_gpu_models/neural_network.h5')\n",
        "xgb_model.save_model('fantasy_ai_gpu_models/xgboost_model.json')\n",
        "\n",
        "import joblib\n",
        "joblib.dump(scaler, 'fantasy_ai_gpu_models/scaler.pkl')\n",
        "\n",
        "metadata = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'total_games': len(games),\n",
        "    'training_samples': len(X_train),\n",
        "    'features': X_train.shape[1],\n",
        "    'accuracy': {\n",
        "        'neural_network': float(nn_test_acc),\n",
        "        'xgboost': float(xgb_test_acc),\n",
        "        'ensemble': float(ensemble_acc),\n",
        "        'optimized_ensemble': float(final_acc)\n",
        "    },\n",
        "    'ensemble_weights': {\n",
        "        'neural_network': float(optimal_weights[0]),\n",
        "        'xgboost': float(optimal_weights[1])\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('fantasy_ai_gpu_models/metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "!cd fantasy_ai_gpu_models && zip -r ../fantasy_ai_gpu_models.zip *\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèÜ FINAL RESULTS:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Neural Network: {nn_test_acc:.2%}\")\n",
        "print(f\"XGBoost: {xgb_test_acc:.2%}\")\n",
        "print(f\"Simple Ensemble: {ensemble_acc:.2%}\")\n",
        "print(f\"Optimized Ensemble: {final_acc:.2%}\")\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download\n",
        "from google.colab import files\n",
        "files.download('fantasy_ai_gpu_models.zip')\n",
        "print(\"\\nüì• Download complete! Save this to your project.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}